{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading product reviews on electronics category\n",
    "e_data = pd.read_csv('review_files/electronics/amazon_electronics_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reading food products reviews\n",
    "f_data = pd.read_csv('review_files/food/food_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null values in both the datasets\n",
    "e_data.dropna(inplace=True)\n",
    "f_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating are in the range of 1 to 5, But Reviews with a score or rating of #3 adds no real value to finding the truth about the product\n",
    "#So, reviews with rating of #3 are removed for the files.\n",
    "f_data[f_data['Score']!=3]\n",
    "e_data[e_data['Rating']!=3]\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the useless columns\n",
    "e_data.drop('Product Name',inplace=True,axis=1)\n",
    "e_data.drop('Brand Name',inplace=True,axis=1)\n",
    "e_data.drop('Price',inplace=True,axis=1)\n",
    "e_data.drop('Review Votes',inplace=True,axis=1)\n",
    "\n",
    "f_data.drop('Id',inplace=True,axis=1)\n",
    "f_data.drop('ProductId',inplace=True,axis=1)\n",
    "f_data.drop('UserId',inplace=True,axis=1)\n",
    "f_data.drop('ProfileName',inplace=True,axis=1)\n",
    "f_data.drop('HelpfulnessNumerator',inplace=True,axis=1)\n",
    "f_data.drop('HelpfulnessDenominator',inplace=True,axis=1)\n",
    "f_data.drop('Time',inplace=True,axis=1)\n",
    "f_data.drop('Summary',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data = e_data.sample(frac=1).reset_index(drop=True)\n",
    "f_data = f_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data = e_data.drop(e_data.index[200000:])\n",
    "f_data = f_data.drop(f_data.index[200000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function which will normalized the review_text\n",
    "def normalize_words(raw_txt):\n",
    "    words = raw_txt.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the normalization function in e_data\n",
    "e_data['clean_reviews'] = e_data['Reviews'].apply(normalize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the normalization function in f_data\n",
    "f_data['clean_reviews'] = f_data['Text'].apply(normalize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting the features columns which we will need in different dataframe and is done for both\n",
    "e_df = e_data[['clean_reviews','Rating']]\n",
    "f_df = f_data[['clean_reviews','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the column names to 'Reviews_text' and 'Rating' in both the dataframe\n",
    "e_df.columns = ['Reviews_text','Rating']\n",
    "f_df.columns = ['Reviews_text','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining or stacking the two dataframe together in one dataframe call clean_file\n",
    "clean_file = f_df.append(e_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the data we have stack looks like this\n",
    "clean_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the data we have right now need to be suffled to prevent learning issues\n",
    "clean_data = clean_file.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The clean data is now ready for classification and apply ML model\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column where reviews greater than #3 are given a score of 1 and less than #3 a score of 0\n",
    "#Of course this is to simplify our understanding and not have to worry about all the review scores as a whole\n",
    "clean_data['new_rate'] = np.where(clean_data['Rating']>3,1,0) #using the numpy where method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to split our data into training sets and test sets using our Reviews_text and Rating column\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(clean_data['Reviews_text'],clean_data['new_rate'],random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert bag of words with count vectorizer is used , which can be0 found in the scikit learn docs\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_Vect = CountVectorizer(ngrams_range=(2,2))\n",
    "count_Vect.fit(X_train)\n",
    "count_Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train data to vectorized data\n",
    "X_train_vec_data = count_Vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally let's apply Logistic regression on the data and create our model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(count_Vect.transform(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can try another means to score our model using the AUC(Area under the curve) score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "pred = model.predict(count_Vect.transform(X_test))\n",
    "print('Score: ',roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(count_Vect.transform(['the product is very good and high quality','Worst product ']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting to SGD classifier to reduce the running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally let's apply SGD regression on the data and create our model\n",
    "start =tm.time()\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import cross_val_predict\n",
    "SGD_model = SGDClassifier(max_iter = 1500,tol=0.0001)\n",
    "SGD_model.fit(X_train_Vec,y_train)\n",
    "end = tm.time() - start\n",
    "#print(\"time taken: %d\"%(end))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = SGD_model.predict(count_Vect.transform(X_test))\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can try another means to score our model using the AUC(Area under the curve) score\n",
    "import time\n",
    "start = time.time()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "pred = SGD_model.predict(count_Vect.transform(X_test))\n",
    "print('Score: ',roc_auc_score(y_test,pred))\n",
    "duration = time.time() - start\n",
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
